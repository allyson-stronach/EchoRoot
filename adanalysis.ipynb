{
 "metadata": {
  "name": "",
  "signature": "sha256:9abde89d2f5e07d70ff328e4a7b28b9b22cd4cb90d1b5f550f9498c9328c4087"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
      "from sklearn import cross_validation\n",
      "from sklearn import metrics\n",
      "\n",
      "from model import session"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze_ad():\n",
      "    dl_list = retrieve_trafficky_text()\n",
      "    dl_list = retrieve_not_trafficky_text(dl_list)\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    x_axis_y_axis = vectorize_ads(dl_list, vectorizer)\n",
      "    classifier = classify_ads(x_axis_y_axis)\n",
      "    test_sample(vectorizer, classifier)\n",
      "    describe_features(vectorizer, classifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def retrieve_trafficky_text():\n",
      "    documents = []\n",
      "    labels = []\n",
      "    ads_text_cmd = \"SELECT ads.text AS text FROM ads_attributes JOIN ads ON ads.id = ads_id WHERE ads_attributes.value IN  ('9292103206', '4142395461', '4146870501') \"\n",
      "    trafficky_text = session.execute(ads_text_cmd)\n",
      "    for text in trafficky_text:\n",
      "        documents.append(text.text)\n",
      "        labels.append('trafficky')\n",
      "    print 'documents length:', len(documents), 'labels length:', len(labels)\n",
      "    dl_list = [documents, labels]\n",
      "\n",
      "    return dl_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def retrieve_not_trafficky_text(dl_list):\n",
      "    ads_text_cmd = \"SELECT ads.text AS text FROM ads_attributes JOIN ads ON ads.id = ads_id WHERE ads_attributes.value IN ('3104623985', '2139840845 ', '8183362736', '6032946322', '4088991922')\"\n",
      "    not_trafficky_text = session.execute(ads_text_cmd)\n",
      "    for text in not_trafficky_text:\n",
      "        dl_list[0].append(text.text)\n",
      "        dl_list[1].append('not trafficky')\n",
      "    print 'documents length:', len(dl_list[0]), 'labels length:', len(dl_list[1])\n",
      "    print 'fraction trafficky:', len([item for item in dl_list[1] if item == 'trafficky'])/240.0\n",
      "\n",
      "    return dl_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_ads(dl_list, vectorizer):\n",
      "    X = vectorizer.fit_transform(dl_list[0])\n",
      "    y = np.array(dl_list[1])\n",
      "    print 'X.shape is:', X.shape, 'y.shape is:', y.shape, 'vectorizer:', vectorizer\n",
      "    x_axis_y_axis = [X, y]\n",
      "\n",
      "    return x_axis_y_axis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_ads(x_axis_y_axis):\n",
      "    classifier = BernoulliNB()\n",
      "    cv = cross_validation.StratifiedKFold(x_axis_y_axis[1],5)\n",
      "    precision=[]\n",
      "    recall=[]\n",
      "    for train, test in cv:\n",
      "        X_train = x_axis_y_axis[0][train]\n",
      "        X_test = x_axis_y_axis[0][test]\n",
      "        y_train = x_axis_y_axis[1][train]\n",
      "        y_test = x_axis_y_axis[1][test]\n",
      "        classifier.fit(X_train, y_train)\n",
      "        y_hat = classifier.predict(X_test)\n",
      "        p,r,_,_ = metrics.precision_recall_fscore_support(y_test, y_hat)x\n",
      "        precision.append(p[1])\n",
      "        recall.append(r[1])\n",
      "    print classifier\n",
      "    print 'precision:',np.average(precision), '+/-', np.std(precision)\n",
      "    print 'recall:', np.average(recall), '+/-', np.std(recall)\n",
      "    \n",
      "    return classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_sample(vectorizer, classifier):\n",
      "    sample = 'Long hair... Long Legs Tall, Busty , Beautiful, Luscious Lips and Curvy Hips<br> All Service<br> In or Out Call<br> Available Days and Nights<br> call 336 307 5841 |'\n",
      "    sample = vectorizer.transform([sample])\n",
      "    classification_new_ad = classifier.predict(sample)\n",
      "    \n",
      "    print classification_new_ad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def describe_features(vectorizer, classifier):\n",
      "    probs=classifier.feature_log_prob_[1]\n",
      "    features=vectorizer.get_feature_names()\n",
      "    \n",
      "    print 'length of probs:', len(probs), 'length of features:', len(features), 'list of most important features:', sorted(zip(probs,features), reverse=True)[:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    analyze_ad()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "documents length: 183 labels length: 183\n",
        "documents length:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 240 labels length: 240\n",
        "fraction trafficky: 0.7625\n",
        "X.shape is: (240, 843) y.shape is: (240,) vectorizer: TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "precision: 0.983474903475 +/- 0.021784275129\n",
        "recall: 0.978078078078 +/- 0.026852959647\n",
        "['trafficky']\n",
        "length of probs: 843 length of features: 843 list of most important features: [(-0.10610650599454807, u'and'), (-0.21645456316341338, u'with'), (-0.22482281283392958, u'to'), (-0.33111747148355342, u'for'), (-0.34050721183339228, u'the'), (-0.35955540680408671, u'me'), (-0.42923532744207638, u'my'), (-0.45006941434491843, u'experience'), (-0.45006941434491843, u'100'), (-0.46065152367545537, u'love')]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}