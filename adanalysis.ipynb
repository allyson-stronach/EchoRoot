{
 "metadata": {
  "name": "",
  "signature": "sha256:3e9f46669823ac75b22d50c6859182c440520e97f1ec97abb24129fbc7a88ca0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
      "from sklearn import cross_validation\n",
      "from sklearn import metrics\n",
      "\n",
      "from model import session"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze_ad():\n",
      "    dl_list = retrieve_trafficky_text()\n",
      "    dl_list = retrieve_not_trafficky_text(dl_list)\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    x_axis_y_axis = vectorize_ads(dl_list, vectorizer)\n",
      "    classifier = classify_ads(x_axis_y_axis)\n",
      "    test_sample(vectorizer, classifier)\n",
      "    describe_features(vectorizer, classifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def retrieve_trafficky_text():\n",
      "    documents = []\n",
      "    labels = []\n",
      "    ads_text_cmd = \"SELECT ads.text AS text FROM ads_attributes JOIN ads ON ads.id = ads_id WHERE ads_attributes.value IN  ('9292103206', '4142395461', '4146870501') \"\n",
      "    trafficky_text = session.execute(ads_text_cmd)\n",
      "    for text in trafficky_text:\n",
      "        documents.append(text.text)\n",
      "        labels.append('trafficky')\n",
      "    print 'documents length:', len(documents), 'labels length:', len(labels)\n",
      "    dl_list = [documents, labels]\n",
      "\n",
      "    return dl_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def retrieve_not_trafficky_text(dl_list):\n",
      "    ads_text_cmd = \"SELECT ads.text AS text FROM ads_attributes JOIN ads ON ads.id = ads_id WHERE ads_attributes.value IN ('3104623985', '2139840845 ', '8183362736', '6032946322', '4088991922')\"\n",
      "    not_trafficky_text = session.execute(ads_text_cmd)\n",
      "    for text in not_trafficky_text:\n",
      "        dl_list[0].append(text.text)\n",
      "        dl_list[1].append('not trafficky')\n",
      "    print 'documents length:', len(dl_list[0]), 'labels length:', len(dl_list[1])\n",
      "    print 'fraction trafficky:', len([item for item in dl_list[1] if item == 'trafficky'])/240.0\n",
      "\n",
      "    return dl_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_ads(dl_list, vectorizer):\n",
      "    X = vectorizer.fit_transform(dl_list[0])\n",
      "    y = np.array(dl_list[1])\n",
      "    print 'X.shape is:', X.shape, 'y.shape is:', y.shape, 'vectorizer:', vectorizer\n",
      "    x_axis_y_axis = [X, y]\n",
      "\n",
      "    return x_axis_y_axis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_ads(x_axis_y_axis):\n",
      "    classifier = BernoulliNB()\n",
      "    cv = cross_validation.StratifiedKFold(x_axis_y_axis[1],5)\n",
      "    precision=[]\n",
      "    recall=[]\n",
      "    print \"cv\", cv\n",
      "    for train, test in cv:\n",
      "        print \"this is train:\", train, \"this is test\", test\n",
      "        X_train = x_axis_y_axis[0][train]\n",
      "        X_test = x_axis_y_axis[0][test]\n",
      "        y_train = x_axis_y_axis[1][train]\n",
      "        y_test = x_axis_y_axis[1][test]\n",
      "        classifier.fit(X_train, y_train)\n",
      "        y_hat = classifier.predict(X_test)\n",
      "        p,r,_,_ = metrics.precision_recall_fscore_support(y_test, y_hat)\n",
      "        precision.append(p[1])\n",
      "        recall.append(r[1])\n",
      "    print classifier\n",
      "    print 'precision:',np.average(precision), '+/-', np.std(precision)\n",
      "    print 'recall:', np.average(recall), '+/-', np.std(recall)\n",
      "    \n",
      "    return classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_sample(vectorizer, classifier):\n",
      "    sample = 'Long hair... Long Legs Tall, Busty , Beautiful, Luscious Lips and Curvy Hips<br> All Service<br> In or Out Call<br> Available Days and Nights<br> call 336 307 5841 |'\n",
      "    sample = vectorizer.transform([sample])\n",
      "    classification_new_ad = classifier.predict(sample)\n",
      "    \n",
      "    print classification_new_ad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def describe_features(vectorizer, classifier):\n",
      "    probs=classifier.feature_log_prob_[1]\n",
      "    features=vectorizer.get_feature_names()\n",
      "    \n",
      "    print 'length of probs:', len(probs), 'length of features:', len(features), 'list of most important features:', sorted(zip(probs,features), reverse=True)[:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    analyze_ad()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "documents length: 183 labels length: 183\n",
        "documents length:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 240 labels length: 240\n",
        "fraction trafficky: 0.7625\n",
        "X.shape is: (240, 843) y.shape is: (240,) vectorizer: TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)\n",
        "cv sklearn.cross_validation.StratifiedKFold(labels=['trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky' 'trafficky'\n",
        " 'trafficky' 'trafficky' 'trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky' 'not trafficky'\n",
        " 'not trafficky' 'not trafficky' 'not trafficky'], n_folds=5, shuffle=False, random_state=None)\n",
        "this is train: [ 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
        "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
        "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
        "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
        " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
        " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
        " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
        " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
        " 181 182 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210\n",
        " 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228\n",
        " 229 230 231 232 233 234 235 236 237 238 239] this is test [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
        "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
        "  36 183 184 185 186 187 188 189 190 191 192 193 194]\n",
        "this is train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
        "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
        "  36  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
        "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
        " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
        " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
        " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
        " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
        " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 207 208 209 210\n",
        " 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228\n",
        " 229 230 231 232 233 234 235 236 237 238 239] this is test [ 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
        "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
        "  73 195 196 197 198 199 200 201 202 203 204 205 206]\n",
        "this is train:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
        "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
        "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
        "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
        "  72  73 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
        " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
        " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
        " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
        " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
        " 199 200 201 202 203 204 205 206 218 219 220 221 222 223 224 225 226 227\n",
        " 228 229 230 231 232 233 234 235 236 237 238 239] this is test [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
        "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
        " 110 207 208 209 210 211 212 213 214 215 216 217]\n",
        "this is train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
        "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
        "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
        "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
        "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
        "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
        " 108 109 110 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
        " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
        " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
        " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
        " 216 217 229 230 231 232 233 234 235 236 237 238 239] this is test [111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
        " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
        " 218 219 220 221 222 223 224 225 226 227 228]\n",
        "this is train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
        "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
        "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
        "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
        "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
        "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
        " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
        " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
        " 144 145 146 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
        " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
        " 216 217 218 219 220 221 222 223 224 225 226 227 228] this is test [147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
        " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
        " 229 230 231 232 233 234 235 236 237 238 239]\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "precision: 0.983474903475 +/- 0.021784275129\n",
        "recall: 0.978078078078 +/- 0.026852959647\n",
        "['trafficky']\n",
        "length of probs: 843 length of features: 843 list of most important features: [(-0.10610650599454807, u'and'), (-0.21645456316341338, u'with'), (-0.22482281283392958, u'to'), (-0.33111747148355342, u'for'), (-0.34050721183339228, u'the'), (-0.35955540680408671, u'me'), (-0.42923532744207638, u'my'), (-0.45006941434491843, u'experience'), (-0.45006941434491843, u'100'), (-0.46065152367545537, u'love')]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}